{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport random\nimport string\nfrom collections import Counter\nfrom itertools import chain\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport umap\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\nfrom nltk.tokenize import WordPunctTokenizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\nfrom tqdm.auto import tqdm as tqdma","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:21:26.811664Z","iopub.execute_input":"2025-02-12T13:21:26.811988Z","iopub.status.idle":"2025-02-12T13:21:26.817099Z","shell.execute_reply.started":"2025-02-12T13:21:26.811955Z","shell.execute_reply":"2025-02-12T13:21:26.816334Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# download the data:\n!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:36.468489Z","iopub.execute_input":"2025-02-12T12:12:36.468816Z","iopub.status.idle":"2025-02-12T12:12:38.245780Z","shell.execute_reply.started":"2025-02-12T12:12:36.468795Z","shell.execute_reply":"2025-02-12T12:12:38.244994Z"}},"outputs":[{"name":"stdout","text":"--2025-02-12 12:12:36--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1 [following]\n--2025-02-12 12:12:36--  https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com/cd/0/inline/Cj--JJssa23M8C1Rp6K9EkGFSrJi3D4IgRGTIT1k-mBoGTMAJnYe7n71wGMwM9EPTDaFtxDpjNK2XMTzqreMrdM9SPgVIeOVux6CBwApP-IRi1OfjPsUvXqttyAX5Nqf_JA/file?dl=1# [following]\n--2025-02-12 12:12:37--  https://uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com/cd/0/inline/Cj--JJssa23M8C1Rp6K9EkGFSrJi3D4IgRGTIT1k-mBoGTMAJnYe7n71wGMwM9EPTDaFtxDpjNK2XMTzqreMrdM9SPgVIeOVux6CBwApP-IRi1OfjPsUvXqttyAX5Nqf_JA/file?dl=1\nResolving uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com (uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\nConnecting to uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com (uce71a02a8d4a033ba2027855659.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 33813903 (32M) [application/binary]\nSaving to: ‘./quora.txt’\n\n./quora.txt         100%[===================>]  32.25M   113MB/s    in 0.3s    \n\n2025-02-12 12:12:38 (113 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data = list(open(\"/kaggle/working/quora.txt\", encoding=\"utf-8\"))\ndata[50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:38.247238Z","iopub.execute_input":"2025-02-12T12:12:38.247465Z","iopub.status.idle":"2025-02-12T12:12:38.443350Z","shell.execute_reply.started":"2025-02-12T12:12:38.247442Z","shell.execute_reply":"2025-02-12T12:12:38.442610Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"What TV shows or books help you read people's body language?\\n\""},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Токенизация – первый шаг. Тексты, с которыми мы работаем, включают в себя пунктуацию, смайлики и прочие нестандартные токены, так что простой str.split не подойдет.\n\nОбратимся к nltk - библиотеку, нашла широкое применеие в области NLP.","metadata":{}},{"cell_type":"code","source":"tokenizer = WordPunctTokenizer()\n\nprint(tokenizer.tokenize(data[50]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:38.444615Z","iopub.execute_input":"2025-02-12T12:12:38.444900Z","iopub.status.idle":"2025-02-12T12:12:38.449493Z","shell.execute_reply.started":"2025-02-12T12:12:38.444867Z","shell.execute_reply":"2025-02-12T12:12:38.448687Z"}},"outputs":[{"name":"stdout","text":"['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data_tok = [\n    tokenizer.tokenize(\n        line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n    )\n    for line in data\n]\ndata_tok = [x for x in data_tok if len(x) >= 3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:38.450277Z","iopub.execute_input":"2025-02-12T12:12:38.450567Z","iopub.status.idle":"2025-02-12T12:12:44.352849Z","shell.execute_reply.started":"2025-02-12T12:12:38.450539Z","shell.execute_reply":"2025-02-12T12:12:44.352213Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Несколько проверок:","metadata":{}},{"cell_type":"code","source":"assert all(\n    isinstance(row, (list, tuple)) for row in data_tok\n), \"please convert each line into a list of tokens (strings)\"\nassert all(\n    all(isinstance(tok, str) for tok in row) for row in data_tok\n), \"please convert each line into a list of tokens (strings)\"\nis_latin = lambda tok: all(\"a\" <= x.lower() <= \"z\" for x in tok)\nassert all(\n    map(lambda l: not is_latin(l) or l.islower(), map(\" \".join, data_tok))\n), \"please make sure to lowercase the data\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:44.353717Z","iopub.execute_input":"2025-02-12T12:12:44.354051Z","iopub.status.idle":"2025-02-12T12:12:45.746124Z","shell.execute_reply.started":"2025-02-12T12:12:44.354016Z","shell.execute_reply":"2025-02-12T12:12:45.745385Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Ниже заданы константы ширины окна контекста и проведена предобработка для построения skip-gram модели.","metadata":{}},{"cell_type":"code","source":"min_count = 5\nwindow_radius = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:45.746847Z","iopub.execute_input":"2025-02-12T12:12:45.747084Z","iopub.status.idle":"2025-02-12T12:12:45.750381Z","shell.execute_reply.started":"2025-02-12T12:12:45.747063Z","shell.execute_reply":"2025-02-12T12:12:45.749596Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"vocabulary_with_counter = Counter(chain.from_iterable(data_tok))\n\nword_count_dict = dict()\nfor word, counter in vocabulary_with_counter.items():\n    if counter >= min_count:\n        word_count_dict[word] = counter\n\nvocabulary = set(word_count_dict.keys())\ndel vocabulary_with_counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:45.752895Z","iopub.execute_input":"2025-02-12T12:12:45.753123Z","iopub.status.idle":"2025-02-12T12:12:46.571727Z","shell.execute_reply.started":"2025-02-12T12:12:45.753105Z","shell.execute_reply":"2025-02-12T12:12:46.571033Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"word_to_index = {word: index for index, word in enumerate(vocabulary)}\nindex_to_word = {index: word for word, index in word_to_index.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:46.573148Z","iopub.execute_input":"2025-02-12T12:12:46.573439Z","iopub.status.idle":"2025-02-12T12:12:46.595120Z","shell.execute_reply.started":"2025-02-12T12:12:46.573415Z","shell.execute_reply":"2025-02-12T12:12:46.594479Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Пары (слово, контекст) на основе доступного датасета сгенерированы ниже.","metadata":{}},{"cell_type":"code","source":"context_pairs = []\n\nfor text in data_tok:\n    for i, central_word in enumerate(text):\n        context_indices = range(\n            max(0, i - window_radius), min(i + window_radius, len(text))\n        )\n        for j in context_indices:\n            if j == i:\n                continue\n            context_word = text[j]\n            if central_word in vocabulary and context_word in vocabulary:\n                context_pairs.append(\n                    (word_to_index[central_word], word_to_index[context_word])\n                )\n\nprint(f\"Generated {len(context_pairs)} pairs of target and context words.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:12:46.595882Z","iopub.execute_input":"2025-02-12T12:12:46.596164Z","iopub.status.idle":"2025-02-12T12:13:13.704521Z","shell.execute_reply.started":"2025-02-12T12:12:46.596133Z","shell.execute_reply":"2025-02-12T12:13:13.703740Z"}},"outputs":[{"name":"stdout","text":"Generated 40220313 pairs of target and context words.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Подзадача №1: subsampling\n\nДля того, чтобы сгладить разницу в частоте встречаемсости слов, необходимо реализовать механизм subsampling'а. Для этого вам необходимо реализовать функцию ниже.","metadata":{}},{"cell_type":"code","source":"def subsample_frequent_words(word_count_dict, threshold=1e-5):\n    \"\"\"\n    Calculates the subsampling probabilities for words based on their frequencies.\n\n    This function is used to determine the probability of keeping a word in the dataset\n    when subsampling frequent words. The method used is inspired by the subsampling approach\n    in Word2Vec, where each word's frequency affects its probability of being kept.\n\n    Parameters:\n    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n    - threshold (float, optional): A threshold parameter used to adjust the frequency of word subsampling.\n                                   Defaults to 1e-5.\n\n    Returns:\n    - dict: A dictionary where keys are words and values are the probabilities of keeping each word.\n\n    Example:\n    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n    >>> subsample_frequent_words(word_counts)\n    {'the': 0.028, 'is': 0.223, 'apple': 1.0}\n    \"\"\"\n\n    # Суммируем все частоты слов\n    total_count = sum(word_count_dict.values())\n    \n    # Инициализируем словарь для вероятностей\n    keep_prob_dict = {}\n\n    # Рассчитываем нормированную частоту для каждого слова\n    for word, count in word_count_dict.items():\n        # Нормированная частота\n        normalized_freq = count / total_count\n        \n        # Вероятность исключения слова (P_drop)\n        p_drop = 1 - math.sqrt(threshold / normalized_freq) if normalized_freq > threshold else 0\n        \n        # Вероятность оставить слово в обучении\n        keep_prob_dict[word] = 1 - p_drop\n    \n    return keep_prob_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.705382Z","iopub.execute_input":"2025-02-12T12:13:13.705700Z","iopub.status.idle":"2025-02-12T12:13:13.710417Z","shell.execute_reply.started":"2025-02-12T12:13:13.705668Z","shell.execute_reply":"2025-02-12T12:13:13.709746Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Подзадача №2: negative sampling\n\nДля более эффективного обучения необходимо не только предсказывать высокие вероятности для слов из контекста, но и предсказывать низкие для слов, не встреченных в контексте. Для этого вам необходимо вычислить вероятность использовать слово в качестве negative sample, реализовав функцию ниже.","metadata":{}},{"cell_type":"code","source":"def get_negative_sampling_prob(word_count_dict):\n    \"\"\"\n    Calculates the negative sampling probabilities for words based on their frequencies.\n\n    This function adjusts the frequency of each word raised to the power of 0.75, which is\n    commonly used in algorithms like Word2Vec to moderate the influence of very frequent words.\n    It then normalizes these adjusted frequencies to ensure they sum to 1, forming a probability\n    distribution used for negative sampling.\n\n    Parameters:\n    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n\n    Returns:\n    - dict: A dictionary where keys are words and values are the probabilities of selecting each word\n            for negative sampling.\n\n    Example:\n    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n    >>> get_negative_sampling_prob(word_counts)\n    {'the': 0.298, 'is': 0.160, 'apple': 0.042}\n    \"\"\"\n    # Корректируем частоты слов по формуле f(w_i) ^ 0.75\n    adjusted_freq_dict = {word: math.pow(count, 0.75) for word, count in word_count_dict.items()}\n\n    # Нормализуем эти значения так, чтобы сумма вероятностей была равна 1\n    total_freq = sum(adjusted_freq_dict.values())\n\n    # Создаем словарь с вероятностями для negative sampling\n    negative_sampling_prob_dict = {\n        word: freq / total_freq for word, freq in adjusted_freq_dict.items()\n    }\n\n    return negative_sampling_prob_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.711111Z","iopub.execute_input":"2025-02-12T12:13:13.711295Z","iopub.status.idle":"2025-02-12T12:13:13.750091Z","shell.execute_reply.started":"2025-02-12T12:13:13.711278Z","shell.execute_reply":"2025-02-12T12:13:13.749392Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Для удобства, преобразуем полученные словари в массивы (т.к. все слова все равно уже пронумерованы).","metadata":{}},{"cell_type":"code","source":"import math\nkeep_prob_dict = subsample_frequent_words(word_count_dict)\nassert keep_prob_dict.keys() == word_count_dict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.750799Z","iopub.execute_input":"2025-02-12T12:13:13.751071Z","iopub.status.idle":"2025-02-12T12:13:13.778693Z","shell.execute_reply.started":"2025-02-12T12:13:13.751050Z","shell.execute_reply":"2025-02-12T12:13:13.777793Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\nassert negative_sampling_prob_dict.keys() == negative_sampling_prob_dict.keys()\nassert np.allclose(sum(negative_sampling_prob_dict.values()), 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.779603Z","iopub.execute_input":"2025-02-12T12:13:13.779904Z","iopub.status.idle":"2025-02-12T12:13:13.809569Z","shell.execute_reply.started":"2025-02-12T12:13:13.779875Z","shell.execute_reply":"2025-02-12T12:13:13.808737Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"keep_prob_array = np.array(\n    [keep_prob_dict[index_to_word[idx]] for idx in range(len(word_to_index))]\n)\nnegative_sampling_prob_array = np.array(\n    [\n        negative_sampling_prob_dict[index_to_word[idx]]\n        for idx in range(len(word_to_index))\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.810316Z","iopub.execute_input":"2025-02-12T12:13:13.810537Z","iopub.status.idle":"2025-02-12T12:13:13.832246Z","shell.execute_reply.started":"2025-02-12T12:13:13.810507Z","shell.execute_reply":"2025-02-12T12:13:13.831514Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Если все прошло успешно, функция ниже поможет вам с генерацией подвыборок (батчей).","metadata":{}},{"cell_type":"code","source":"def generate_batch_with_neg_samples(\n    context_pairs,\n    batch_size,\n    keep_prob_array,\n    word_to_index,\n    num_negatives,\n    negative_sampling_prob_array,\n):\n    batch = []\n    neg_samples = []\n\n    while len(batch) < batch_size:\n        center, context = random.choice(context_pairs)\n        if random.random() < keep_prob_array[center]:\n            batch.append((center, context))\n            neg_sample = np.random.choice(\n                range(len(negative_sampling_prob_array)),\n                size=num_negatives,\n                p=negative_sampling_prob_array,\n            )\n            neg_samples.append(neg_sample)\n    batch = np.array(batch)\n    neg_samples = np.vstack(neg_samples)\n    return batch, neg_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.833034Z","iopub.execute_input":"2025-02-12T12:13:13.833223Z","iopub.status.idle":"2025-02-12T12:13:13.845263Z","shell.execute_reply.started":"2025-02-12T12:13:13.833206Z","shell.execute_reply":"2025-02-12T12:13:13.844422Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"batch_size = 4\nnum_negatives = 15\nbatch, neg_samples = generate_batch_with_neg_samples(\n    context_pairs,\n    batch_size,\n    keep_prob_array,\n    word_to_index,\n    num_negatives,\n    negative_sampling_prob_array,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.846176Z","iopub.execute_input":"2025-02-12T12:13:13.846445Z","iopub.status.idle":"2025-02-12T12:13:13.868818Z","shell.execute_reply.started":"2025-02-12T12:13:13.846418Z","shell.execute_reply":"2025-02-12T12:13:13.868000Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class SkipGramModelWithNegSampling(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, center_words, pos_context_words, neg_context_words):\n        # Извлекаем векторы эмбеддингов для центральных и контекстных слов\n        center_embeds = self.center_embeddings(center_words).to(device)\n        pos_context_embeds = self.context_embeddings(pos_context_words).to(device)  # Векторы для положительного контекста\n        neg_context_embeds = self.context_embeddings(neg_context_words).to(device)  # Векторы для отрицательных контекстов\n\n        # Положительные скоры: скалярное произведение вектора центрального слова и контекстного слова\n        pos_scores = torch.sum(center_embeds * pos_context_embeds, dim=1)\n        # Отрицательные скоры: скалярное произведение для каждого отрицательного контекста\n        neg_scores = torch.sum(center_embeds.unsqueeze(1) * neg_context_embeds, dim=2)\n        \n        return pos_scores, neg_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.869643Z","iopub.execute_input":"2025-02-12T12:13:13.869939Z","iopub.status.idle":"2025-02-12T12:13:13.874850Z","shell.execute_reply.started":"2025-02-12T12:13:13.869892Z","shell.execute_reply":"2025-02-12T12:13:13.874182Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.875669Z","iopub.execute_input":"2025-02-12T12:13:13.875992Z","iopub.status.idle":"2025-02-12T12:13:13.958483Z","shell.execute_reply.started":"2025-02-12T12:13:13.875962Z","shell.execute_reply":"2025-02-12T12:13:13.957533Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"vocab_size = len(word_to_index)\nembedding_dim = 32\nnum_negatives = 15\n\nmodel = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.05)\nlr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=150)\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:13.959285Z","iopub.execute_input":"2025-02-12T12:13:13.959496Z","iopub.status.idle":"2025-02-12T12:13:15.962839Z","shell.execute_reply.started":"2025-02-12T12:13:13.959473Z","shell.execute_reply":"2025-02-12T12:13:15.961976Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"params_counter = 0\nfor weights in model.parameters():\n    params_counter += weights.shape.numel()\nassert params_counter == len(word_to_index) * embedding_dim * 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:15.963685Z","iopub.execute_input":"2025-02-12T12:13:15.964144Z","iopub.status.idle":"2025-02-12T12:13:15.968255Z","shell.execute_reply.started":"2025-02-12T12:13:15.964109Z","shell.execute_reply":"2025-02-12T12:13:15.967316Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train_skipgram_with_neg_sampling(\n    model,\n    context_pairs,\n    keep_prob_array,\n    word_to_index,\n    batch_size,\n    num_negatives,\n    negative_sampling_prob_array,\n    steps,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    device=device,\n):\n    pos_labels = torch.ones(batch_size).to(device)\n    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n    loss_history = []\n    for step in tqdma(range(steps)):\n        batch, neg_samples = generate_batch_with_neg_samples(\n            context_pairs,\n            batch_size,\n            keep_prob_array,\n            word_to_index,\n            num_negatives,\n            negative_sampling_prob_array,\n        )\n        center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long).to(\n            device\n        )\n        pos_context_words = torch.tensor(\n            [pair[1] for pair in batch], dtype=torch.long\n        ).to(device)\n        neg_context_words = torch.tensor(neg_samples, dtype=torch.long).to(device)\n\n        optimizer.zero_grad()\n        pos_scores, neg_scores = model(\n            center_words, pos_context_words, neg_context_words\n        )\n\n        loss_pos = criterion(pos_scores, pos_labels)\n        loss_neg = criterion(neg_scores, neg_labels)\n\n        loss = loss_pos + loss_neg\n        loss.backward()\n        optimizer.step()\n\n        loss_history.append(loss.item())\n        lr_scheduler.step(loss_history[-1])\n\n        if step % 100 == 0:\n            print(\n                f\"Step {step}, Loss: {np.mean(loss_history[-100:])}, learning rate: {lr_scheduler._last_lr}\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:15.969041Z","iopub.execute_input":"2025-02-12T12:13:15.969271Z","iopub.status.idle":"2025-02-12T12:13:15.985203Z","shell.execute_reply.started":"2025-02-12T12:13:15.969251Z","shell.execute_reply":"2025-02-12T12:13:15.984460Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"steps = 2500\nbatch_size = 512\ntrain_skipgram_with_neg_sampling(\n    model,\n    context_pairs,\n    keep_prob_array,\n    word_to_index,\n    batch_size,\n    num_negatives,\n    negative_sampling_prob_array,\n    steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:13:15.987518Z","iopub.execute_input":"2025-02-12T12:13:15.987710Z","iopub.status.idle":"2025-02-12T13:03:48.266717Z","shell.execute_reply.started":"2025-02-12T12:13:15.987692Z","shell.execute_reply":"2025-02-12T13:03:48.265654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376c7a6cbe6f4224a1eb65291ba2574b"}},"metadata":{}},{"name":"stdout","text":"Step 0, Loss: 4.486419677734375, learning rate: [0.05]\nStep 100, Loss: 3.4633967900276184, learning rate: [0.05]\nStep 200, Loss: 2.559452996253967, learning rate: [0.05]\nStep 300, Loss: 2.268398609161377, learning rate: [0.05]\nStep 400, Loss: 2.1213196647167205, learning rate: [0.05]\nStep 500, Loss: 2.060541056394577, learning rate: [0.05]\nStep 600, Loss: 2.0341793859004973, learning rate: [0.05]\nStep 700, Loss: 2.0232198631763456, learning rate: [0.025]\nStep 800, Loss: 1.9393684089183807, learning rate: [0.025]\nStep 900, Loss: 1.8481394338607788, learning rate: [0.025]\nStep 1000, Loss: 1.8202050507068634, learning rate: [0.0125]\nStep 1100, Loss: 1.766991500854492, learning rate: [0.0125]\nStep 1200, Loss: 1.7058832991123198, learning rate: [0.0125]\nStep 1300, Loss: 1.6823325407505036, learning rate: [0.0125]\nStep 1400, Loss: 1.667913794517517, learning rate: [0.00625]\nStep 1500, Loss: 1.6221528375148773, learning rate: [0.00625]\nStep 1600, Loss: 1.5961763679981231, learning rate: [0.00625]\nStep 1700, Loss: 1.5906234717369079, learning rate: [0.003125]\nStep 1800, Loss: 1.5821360504627229, learning rate: [0.003125]\nStep 1900, Loss: 1.5699185144901275, learning rate: [0.0015625]\nStep 2000, Loss: 1.5647952270507812, learning rate: [0.0015625]\nStep 2100, Loss: 1.5688249599933624, learning rate: [0.00078125]\nStep 2200, Loss: 1.5645032978057862, learning rate: [0.000390625]\nStep 2300, Loss: 1.5481578600406647, learning rate: [0.000390625]\nStep 2400, Loss: 1.5556933283805847, learning rate: [0.000390625]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Наконец, используйте полученную матрицу весов в качестве матрицы в векторными представлениями слов. Рекомендуем использовать для сдачи матрицу, которая отвечала за слова из контекста (т.е. декодера).","metadata":{}},{"cell_type":"code","source":"_model_parameters = model.parameters()\nembedding_matrix_center = next(\n    _model_parameters\n).detach()  # Assuming that first matrix was for central word\nembedding_matrix_context = next(\n    _model_parameters\n).detach()  # Assuming that second matrix was for context word","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:03:48.267951Z","iopub.execute_input":"2025-02-12T13:03:48.268288Z","iopub.status.idle":"2025-02-12T13:03:48.273410Z","shell.execute_reply.started":"2025-02-12T13:03:48.268252Z","shell.execute_reply":"2025-02-12T13:03:48.272635Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def get_word_vector(word, embedding_matrix, word_to_index=word_to_index):\n    return embedding_matrix[word_to_index[word]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:06:24.572859Z","iopub.execute_input":"2025-02-12T13:06:24.573181Z","iopub.status.idle":"2025-02-12T13:06:24.577129Z","shell.execute_reply.started":"2025-02-12T13:06:24.573156Z","shell.execute_reply":"2025-02-12T13:06:24.576228Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"Простые проверки:","metadata":{}},{"cell_type":"code","source":"similarity_1 = F.cosine_similarity(\n    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n    get_word_vector(\"laptop\", embedding_matrix_context)[None, :],\n)\nsimilarity_2 = F.cosine_similarity(\n    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n    get_word_vector(\"macbook\", embedding_matrix_context)[None, :],\n)\nassert similarity_1 > similarity_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:10:40.633748Z","iopub.execute_input":"2025-02-12T13:10:40.634056Z","iopub.status.idle":"2025-02-12T13:10:40.638960Z","shell.execute_reply.started":"2025-02-12T13:10:40.634031Z","shell.execute_reply":"2025-02-12T13:10:40.638211Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"similarity_1 = F.cosine_similarity(\n    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n    get_word_vector(\"apple\", embedding_matrix_context)[None, :],\n)\nsimilarity_2 = F.cosine_similarity(\n    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n    get_word_vector(\"acer\", embedding_matrix_context)[None, :],\n)\nassert similarity_1 > similarity_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:11:19.908482Z","iopub.execute_input":"2025-02-12T13:11:19.908758Z","iopub.status.idle":"2025-02-12T13:11:19.913753Z","shell.execute_reply.started":"2025-02-12T13:11:19.908735Z","shell.execute_reply":"2025-02-12T13:11:19.913074Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"Наконец, взглянем на ближайшие по косинусной мере слова. Функция реализована ниже.","metadata":{}},{"cell_type":"code","source":"def find_nearest(word, embedding_matrix, word_to_index=word_to_index, k=10):\n    word_vector = get_word_vector(word, embedding_matrix)[None, :]\n    dists = F.cosine_similarity(embedding_matrix, word_vector)\n    index_sorted = torch.argsort(dists)\n    top_k = index_sorted[-k:]\n    \n    return [(index_to_word[x], dists[x].item()) for x in top_k.cpu().numpy()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:14:01.103714Z","iopub.execute_input":"2025-02-12T13:14:01.104042Z","iopub.status.idle":"2025-02-12T13:14:01.108487Z","shell.execute_reply.started":"2025-02-12T13:14:01.104013Z","shell.execute_reply":"2025-02-12T13:14:01.107599Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"find_nearest(\"python\", embedding_matrix_context, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:14:02.514132Z","iopub.execute_input":"2025-02-12T13:14:02.514408Z","iopub.status.idle":"2025-02-12T13:14:02.520910Z","shell.execute_reply.started":"2025-02-12T13:14:02.514385Z","shell.execute_reply":"2025-02-12T13:14:02.519994Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[('adfly', 0.5955861210823059),\n ('inuit', 0.5980352163314819),\n ('cranky', 0.6008378267288208),\n ('except', 0.604779839515686),\n ('runescape', 0.6069560647010803),\n ('sigmoid', 0.6190297603607178),\n ('rope', 0.6442843079566956),\n ('messaging', 0.6486614942550659),\n ('desperation', 0.6745316982269287),\n ('python', 0.9999999403953552)]"},"metadata":{}}],"execution_count":36}]}